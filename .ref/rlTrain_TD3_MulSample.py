#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Sun Jan 25 23:51:43 2026

@author: ryne
"""

#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é€šç”¨åŒ–çš„ç”µåŠ›ç³»ç»Ÿæ— åŠŸä¼˜åŒ–TD3å¼ºåŒ–å­¦ä¹ è®­ç»ƒç¨‹åºï¼ˆçº¯NumPyç‰ˆæœ¬ï¼Œä¿®å¤ç»´åº¦ä¸åŒ¹é…ï¼‰
æ”¯æŒä»æŒ‡å®šExcelæ–‡ä»¶è¯»å–é…ç½®å‚æ•°å’Œè®­ç»ƒæ•°æ®
æ”¯æŒå¤šæ ·æœ¬åˆ†å±‚è®­ç»ƒï¼ˆæ¯ä¸ªæ ·æœ¬è¿ç»­è®­ç»ƒå¤šä¸ªepisodeï¼‰
è‡ªåŠ¨è®¡ç®—åŸºå‡†ç½‘æŸç‡å¹¶è¿›è¡Œå½’ä¸€åŒ–è®­ç»ƒ
"""

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import MaxNLocator
import copy
import random
import datetime
import os
import glob
import pandas as pd
from typing import List, Tuple, Dict, Optional
from collections import deque  # å¯¹é½PyTorchçš„ç»éªŒå›æ”¾å®¹å™¨

# -------------------------- å…¨å±€é…ç½®ï¼ˆå¯¹é½PyTorchï¼‰ --------------------------
DTYPE = np.float32  # ç»Ÿä¸€æ•°æ®ç±»å‹ä¸ºfloat32ï¼ˆPyTorché»˜è®¤ï¼‰
EPS = 1e-8          # Adamä¼˜åŒ–å™¨epsilonï¼ˆå¯¹é½PyTorchï¼‰
BETA1 = 0.9         # Adam beta1
BETA2 = 0.999       # Adam beta2
MAX_GRAD_NORM = 1.0 # æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°

# -------------------------- åŸºç¡€å·¥å…·å‡½æ•°ï¼ˆå¯¹é½PyTorchï¼‰ --------------------------
def relu(x: np.ndarray) -> np.ndarray:
    """ReLUæ¿€æ´»å‡½æ•°ï¼ˆå¯¹é½PyTorchçš„nn.ReLUï¼‰"""
    return np.maximum(0, x).astype(DTYPE)

def tanh(x: np.ndarray) -> np.ndarray:
    """Tanhæ¿€æ´»å‡½æ•°ï¼ˆå¯¹é½PyTorchçš„nn.Tanhï¼‰"""
    return np.tanh(x).astype(DTYPE)

def xavier_uniform(shape: Tuple[int, int], gain: float = 1.0) -> np.ndarray:
    """ä¸¥æ ¼å¯¹é½PyTorchçš„nn.init.xavier_uniform_åˆå§‹åŒ–"""
    fan_in, fan_out = shape[0], shape[1]
    limit = gain * np.sqrt(6.0 / (fan_in + fan_out))
    return np.random.uniform(-limit, limit, shape).astype(DTYPE)

def clip_grad_norm(grads: Dict[str, np.ndarray], max_norm: float = MAX_GRAD_NORM) -> Dict[str, np.ndarray]:
    """ä¸¥æ ¼å¯¹é½PyTorchçš„torch.nn.utils.clip_grad_norm_"""
    # è®¡ç®—æ€»L2èŒƒæ•°
    total_norm = 0.0
    for g in grads.values():
        total_norm += np.sum(np.square(g))
    total_norm = np.sqrt(total_norm)
    
    # è®¡ç®—è£å‰ªç³»æ•°
    clip_coef = max_norm / (total_norm + EPS)
    if clip_coef < 1.0:
        for k in grads.keys():
            grads[k] = grads[k] * clip_coef
    
    return grads

# -------------------------- Adamä¼˜åŒ–å™¨ï¼ˆä¸¥æ ¼å¯¹é½PyTorchï¼‰ --------------------------
class AdamOptimizer:
    """çº¯NumPyå®ç°çš„Adamä¼˜åŒ–å™¨ï¼ˆå®Œå…¨å¯¹é½PyTorchçš„optim.Adamï¼‰"""
    def __init__(self, params: Dict[str, np.ndarray], lr: float = 1e-3):
        self.lr = lr
        self.beta1 = BETA1
        self.beta2 = BETA2
        self.eps = EPS
        self.t = 0
        
        # åˆå§‹åŒ–åŠ¨é‡å’ŒäºŒé˜¶åŠ¨é‡ï¼ˆä¸¥æ ¼åŒ¹é…å‚æ•°å½¢çŠ¶å’Œç±»å‹ï¼‰
        self.m = {k: np.zeros_like(v, dtype=DTYPE) for k, v in params.items()}
        self.v = {k: np.zeros_like(v, dtype=DTYPE) for k, v in params.items()}
    
    def step(self, params: Dict[str, np.ndarray], grads: Dict[str, np.ndarray]):
        """æ‰§è¡Œä¸€æ­¥ä¼˜åŒ–ï¼ˆå®Œå…¨å¯¹é½PyTorchçš„Adamæ›´æ–°é€»è¾‘ï¼‰"""
        self.t += 1
        for k in params.keys():
            if k not in grads:
                raise KeyError(f"æ¢¯åº¦å­—å…¸ç¼ºå°‘å‚æ•°{k}ï¼Œå¯ç”¨æ¢¯åº¦ï¼š{list(grads.keys())}")
            
            grad = grads[k].astype(DTYPE)
            
            # æ›´æ–°ä¸€é˜¶åŠ¨é‡
            self.m[k] = self.beta1 * self.m[k] + (1 - self.beta1) * grad
            # æ›´æ–°äºŒé˜¶åŠ¨é‡
            self.v[k] = self.beta2 * self.v[k] + (1 - self.beta2) * np.square(grad)
            
            # åå·®ä¿®æ­£
            m_hat = self.m[k] / (1 - np.power(self.beta1, self.t))
            v_hat = self.v[k] / (1 - np.power(self.beta2, self.t))
            
            # å‚æ•°æ›´æ–°ï¼ˆå¯¹é½PyTorchçš„æ›´æ–°å…¬å¼ï¼‰
            params[k] = params[k] - self.lr * m_hat / (np.sqrt(v_hat) + self.eps)
        
        return params

# -------------------------- ç½‘ç»œå®šä¹‰ï¼ˆå¯¹é½PyTorchï¼‰ --------------------------
class ActorNetwork:
    """çº¯NumPyå®ç°çš„Actorç½‘ç»œï¼ˆå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬ï¼‰"""
    def __init__(self, state_dim: int, action_dim: int, max_action: float):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.max_action = max_action
        
        # ä¸¥æ ¼å¯¹é½PyTorchçš„nn.Sequentialåˆå§‹åŒ–
        self.params = {
            "w1": xavier_uniform((state_dim, 128), gain=1.0),
            "b1": np.zeros(128, dtype=DTYPE),
            "w2": xavier_uniform((128, 64), gain=1.0),
            "b2": np.zeros(64, dtype=DTYPE),
            "w3": xavier_uniform((64, 32), gain=1.0),
            "b3": np.zeros(32, dtype=DTYPE),
            "w4": xavier_uniform((32, action_dim), gain=1.0),
            "b4": np.zeros(action_dim, dtype=DTYPE)
        }
    
    def forward(self, state: np.ndarray) -> np.ndarray:
        """å‰å‘ä¼ æ’­ï¼ˆä¸¥æ ¼å¯¹é½PyTorchçš„å‰å‘é€»è¾‘ï¼‰"""
        state = state.astype(DTYPE)
        x = relu(np.dot(state, self.params["w1"]) + self.params["b1"])
        x = relu(np.dot(x, self.params["w2"]) + self.params["b2"])
        x = relu(np.dot(x, self.params["w3"]) + self.params["b3"])
        x = tanh(np.dot(x, self.params["w4"]) + self.params["b4"])
        return self.max_action * x
    
    def get_params(self) -> Dict[str, np.ndarray]:
        """è·å–å‚æ•°å‰¯æœ¬ï¼ˆå¯¹é½PyTorchçš„state_dictï¼‰"""
        return {k: v.copy() for k, v in self.params.items()}
    
    def set_params(self, params: Dict[str, np.ndarray]):
        """è®¾ç½®å‚æ•°ï¼ˆå¯¹é½PyTorchçš„load_state_dictï¼‰"""
        for k, v in params.items():
            self.params[k] = v.astype(DTYPE)

class CriticNetwork:
    """çº¯NumPyå®ç°çš„åŒCriticç½‘ç»œï¼ˆå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬ï¼‰"""
    def __init__(self, state_dim: int, action_dim: int):
        self.input_dim = state_dim + action_dim
        
        # ç½‘ç»œ1å‚æ•°ï¼ˆå¯¹é½PyTorchçš„network1ï¼‰
        self.params1 = {
            "w1": xavier_uniform((self.input_dim, 128), gain=1.0),
            "b1": np.zeros(128, dtype=DTYPE),
            "w2": xavier_uniform((128, 64), gain=1.0),
            "b2": np.zeros(64, dtype=DTYPE),
            "w3": xavier_uniform((64, 32), gain=1.0),
            "b3": np.zeros(32, dtype=DTYPE),
            "w4": xavier_uniform((32, 1), gain=1.0),
            "b4": np.zeros(1, dtype=DTYPE)
        }
        
        # ç½‘ç»œ2å‚æ•°ï¼ˆå¯¹é½PyTorchçš„network2ï¼‰
        self.params2 = {
            "w1": xavier_uniform((self.input_dim, 128), gain=1.0),
            "b1": np.zeros(128, dtype=DTYPE),
            "w2": xavier_uniform((128, 64), gain=1.0),
            "b2": np.zeros(64, dtype=DTYPE),
            "w3": xavier_uniform((64, 32), gain=1.0),
            "b3": np.zeros(32, dtype=DTYPE),
            "w4": xavier_uniform((32, 1), gain=1.0),
            "b4": np.zeros(1, dtype=DTYPE)
        }
    
    def forward(self, state: np.ndarray, action: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """å‰å‘ä¼ æ’­ï¼ˆè¿”å›ä¸¤ä¸ªQå€¼ï¼Œå¯¹é½PyTorchï¼‰"""
        state = state.astype(DTYPE)
        action = action.astype(DTYPE)
        x = np.concatenate([state, action], axis=1)
        
        # ç½‘ç»œ1å‰å‘
        q1 = relu(np.dot(x, self.params1["w1"]) + self.params1["b1"])
        q1 = relu(np.dot(q1, self.params1["w2"]) + self.params1["b2"])
        q1 = relu(np.dot(q1, self.params1["w3"]) + self.params1["b3"])
        q1 = np.dot(q1, self.params1["w4"]) + self.params1["b4"]
        
        # ç½‘ç»œ2å‰å‘
        q2 = relu(np.dot(x, self.params2["w1"]) + self.params2["b1"])
        q2 = relu(np.dot(q2, self.params2["w2"]) + self.params2["b2"])
        q2 = relu(np.dot(q2, self.params2["w3"]) + self.params2["b3"])
        q2 = np.dot(q2, self.params2["w4"]) + self.params2["b4"]
        
        return q1, q2
    
    def Q1(self, state: np.ndarray, action: np.ndarray) -> np.ndarray:
        """ä»…è¿”å›ç¬¬ä¸€ä¸ªQç½‘ç»œè¾“å‡ºï¼ˆå¯¹é½PyTorchï¼‰"""
        state = state.astype(DTYPE)
        action = action.astype(DTYPE)
        x = np.concatenate([state, action], axis=1)
        q1 = relu(np.dot(x, self.params1["w1"]) + self.params1["b1"])
        q1 = relu(np.dot(q1, self.params1["w2"]) + self.params1["b2"])
        q1 = relu(np.dot(q1, self.params1["w3"]) + self.params1["b3"])
        q1 = np.dot(q1, self.params1["w4"]) + self.params1["b4"]
        return q1
    
    def get_params(self) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
        """è·å–ä¸¤ä¸ªç½‘ç»œçš„å‚æ•°å‰¯æœ¬"""
        return (
            {k: v.copy() for k, v in self.params1.items()},
            {k: v.copy() for k, v in self.params2.items()}
        )
    
    def set_params(self, params1: Dict[str, np.ndarray], params2: Dict[str, np.ndarray]):
        """è®¾ç½®ç½‘ç»œå‚æ•°"""
        for k, v in params1.items():
            self.params1[k] = v.astype(DTYPE)
        for k, v in params2.items():
            self.params2[k] = v.astype(DTYPE)

# -------------------------- TD3ç®—æ³•ï¼ˆå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬ï¼‰ --------------------------
class TD3:
    def __init__(self, state_dim: int, action_dim: int, max_action: float):
        print(f"ä½¿ç”¨è®¾å¤‡: CPU (NumPy float32)")
        
        # åˆå§‹åŒ–ä¸»ç½‘ç»œ
        self.actor = ActorNetwork(state_dim, action_dim, max_action)
        self.critic = CriticNetwork(state_dim, action_dim)
        
        # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œå¹¶å¤åˆ¶å‚æ•°ï¼ˆå¯¹é½PyTorchçš„load_state_dictï¼‰
        self.actor_target = ActorNetwork(state_dim, action_dim, max_action)
        self.actor_target.set_params(self.actor.get_params())
        
        self.critic_target = CriticNetwork(state_dim, action_dim)
        critic_params1, critic_params2 = self.critic.get_params()
        self.critic_target.set_params(critic_params1, critic_params2)
        
        # åˆå§‹åŒ–ä¼˜åŒ–å™¨ï¼ˆå¯¹é½PyTorchçš„å­¦ä¹ ç‡ï¼‰
        self.actor_optimizer = AdamOptimizer(self.actor.get_params(), lr=1e-4)
        # æ‰å¹³åŒ–Criticå‚æ•°ç”¨äºä¼˜åŒ–å™¨
        critic_flat_params = self._flatten_critic_params()
        self.critic_optimizer = AdamOptimizer(critic_flat_params, lr=1e-3)
        
        # è¶…å‚æ•°ï¼ˆå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬ï¼‰
        self.max_action = max_action
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.replay_buffer = deque(maxlen=50000)  # æ”¹ç”¨dequeï¼ˆå¯¹é½PyTorchï¼‰
        self.batch_size = 128
        self.policy_noise = 0.2
        self.noise_clip = 0.5
        self.policy_freq = 2
        self.total_it = 0
    
    def _flatten_critic_params(self) -> Dict[str, np.ndarray]:
        """æ‰å¹³åŒ–Criticå‚æ•°ï¼ˆç”¨äºä¼˜åŒ–å™¨ï¼‰"""
        params1, params2 = self.critic.get_params()
        flat_params = {}
        # ç½‘ç»œ1å‚æ•°
        for k, v in params1.items():
            flat_params[f"c1_{k}"] = v
        # ç½‘ç»œ2å‚æ•°
        for k, v in params2.items():
            flat_params[f"c2_{k}"] = v
        return flat_params
    
    def _unflatten_critic_params(self, flat_params: Dict[str, np.ndarray]) -> Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:
        """è§£æ‰å¹³åŒ–Criticå‚æ•°"""
        params1 = {}
        params2 = {}
        for k, v in flat_params.items():
            if k.startswith("c1_"):
                params1[k[3:]] = v
            elif k.startswith("c2_"):
                params2[k[3:]] = v
        return params1, params2
    
    def select_action(self, state: np.ndarray, exploration_noise: float = 0.1) -> np.ndarray:
        """é€‰æ‹©åŠ¨ä½œï¼ˆå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„å™ªå£°é€»è¾‘ï¼‰"""
        state = state.reshape(1, -1).astype(DTYPE)
        action = self.actor.forward(state).flatten()
        
        if exploration_noise != 0:
            # å¯¹é½PyTorchçš„torch.randn_like + clamp
            noise = np.random.normal(0, exploration_noise, size=self.action_dim).astype(DTYPE)
            noise = np.clip(noise, -self.noise_clip, self.noise_clip)
            action = np.clip(action + noise, -self.max_action, self.max_action)
        
        return action
    
    def _compute_critic_grads(self, state_batch: np.ndarray, action_batch: np.ndarray, target_Q: np.ndarray) -> Dict[str, np.ndarray]:
        """è®¡ç®—Criticæ¢¯åº¦ï¼ˆä¸¥æ ¼å¤åˆ»PyTorchè‡ªåŠ¨å¾®åˆ†ï¼‰"""
        state_batch = state_batch.astype(DTYPE)
        action_batch = action_batch.astype(DTYPE)
        target_Q = target_Q.astype(DTYPE)
        
        # å‰å‘ä¼ æ’­å¹¶ä¿å­˜ä¸­é—´ç»“æœ
        x = np.concatenate([state_batch, action_batch], axis=1)
        
        # ç½‘ç»œ1å‰å‘
        h1_1 = relu(np.dot(x, self.critic.params1["w1"]) + self.critic.params1["b1"])
        h2_1 = relu(np.dot(h1_1, self.critic.params1["w2"]) + self.critic.params1["b2"])
        h3_1 = relu(np.dot(h2_1, self.critic.params1["w3"]) + self.critic.params1["b3"])
        q1 = np.dot(h3_1, self.critic.params1["w4"]) + self.critic.params1["b4"]
        
        # ç½‘ç»œ2å‰å‘
        h1_2 = relu(np.dot(x, self.critic.params2["w1"]) + self.critic.params2["b1"])
        h2_2 = relu(np.dot(h1_2, self.critic.params2["w2"]) + self.critic.params2["b2"])
        h3_2 = relu(np.dot(h2_2, self.critic.params2["w3"]) + self.critic.params2["b3"])
        q2 = np.dot(h3_2, self.critic.params2["w4"]) + self.critic.params2["b4"]
        
        # MSEæŸå¤±æ¢¯åº¦ï¼ˆå¯¹é½PyTorchçš„nn.MSELossï¼‰
        grad_q1 = 2 * (q1 - target_Q) / self.batch_size
        grad_q2 = 2 * (q2 - target_Q) / self.batch_size
        
        # -------------------------- ç½‘ç»œ1æ¢¯åº¦è®¡ç®— --------------------------
        # w4/b4æ¢¯åº¦
        grad_w4_1 = np.dot(h3_1.T, grad_q1)
        grad_b4_1 = np.sum(grad_q1, axis=0)
        
        # w3/b3æ¢¯åº¦
        grad_h3_1 = np.dot(grad_q1, self.critic.params1["w4"].T)
        grad_h3_1 = grad_h3_1 * (h3_1 > 0).astype(DTYPE)  # ReLUæ¢¯åº¦
        grad_w3_1 = np.dot(h2_1.T, grad_h3_1)
        grad_b3_1 = np.sum(grad_h3_1, axis=0)
        
        # w2/b2æ¢¯åº¦
        grad_h2_1 = np.dot(grad_h3_1, self.critic.params1["w3"].T)
        grad_h2_1 = grad_h2_1 * (h2_1 > 0).astype(DTYPE)
        grad_w2_1 = np.dot(h1_1.T, grad_h2_1)
        grad_b2_1 = np.sum(grad_h2_1, axis=0)
        
        # w1/b1æ¢¯åº¦
        grad_h1_1 = np.dot(grad_h2_1, self.critic.params1["w2"].T)
        grad_h1_1 = grad_h1_1 * (h1_1 > 0).astype(DTYPE)
        grad_w1_1 = np.dot(x.T, grad_h1_1)
        grad_b1_1 = np.sum(grad_h1_1, axis=0)
        
        # -------------------------- ç½‘ç»œ2æ¢¯åº¦è®¡ç®— --------------------------
        # w4/b4æ¢¯åº¦
        grad_w4_2 = np.dot(h3_2.T, grad_q2)
        grad_b4_2 = np.sum(grad_q2, axis=0)
        
        # w3/b3æ¢¯åº¦
        grad_h3_2 = np.dot(grad_q2, self.critic.params2["w4"].T)
        grad_h3_2 = grad_h3_2 * (h3_2 > 0).astype(DTYPE)
        grad_w3_2 = np.dot(h2_2.T, grad_h3_2)
        grad_b3_2 = np.sum(grad_h3_2, axis=0)
        
        # w2/b2æ¢¯åº¦
        grad_h2_2 = np.dot(grad_h3_2, self.critic.params2["w3"].T)
        grad_h2_2 = grad_h2_2 * (h2_2 > 0).astype(DTYPE)
        grad_w2_2 = np.dot(h1_2.T, grad_h2_2)
        grad_b2_2 = np.sum(grad_h2_2, axis=0)
        
        # w1/b1æ¢¯åº¦
        grad_h1_2 = np.dot(grad_h2_2, self.critic.params2["w2"].T)
        grad_h1_2 = grad_h1_2 * (h1_2 > 0).astype(DTYPE)
        grad_w1_2 = np.dot(x.T, grad_h1_2)
        grad_b1_2 = np.sum(grad_h1_2, axis=0)
        
        # æ•´åˆæ¢¯åº¦ï¼ˆæ‰å¹³åŒ–ï¼Œä¸å‚æ•°é”®åŒ¹é…ï¼‰
        grads = {
            # ç½‘ç»œ1æ¢¯åº¦
            "c1_w1": grad_w1_1, "c1_b1": grad_b1_1,
            "c1_w2": grad_w2_1, "c1_b2": grad_b2_1,
            "c1_w3": grad_w3_1, "c1_b3": grad_b3_1,
            "c1_w4": grad_w4_1, "c1_b4": grad_b4_1,
            # ç½‘ç»œ2æ¢¯åº¦
            "c2_w1": grad_w1_2, "c2_b1": grad_b1_2,
            "c2_w2": grad_w2_2, "c2_b2": grad_b2_2,
            "c2_w3": grad_w3_2, "c2_b3": grad_b3_2,
            "c2_w4": grad_w4_2, "c2_b4": grad_b4_2,
        }
        
        return grads
    
    def _compute_actor_grads(self, state_batch: np.ndarray) -> Dict[str, np.ndarray]:
        """è®¡ç®—Actoræ¢¯åº¦ï¼ˆä¿®å¤ç»´åº¦ä¸åŒ¹é…é—®é¢˜ï¼‰"""
        state_batch = state_batch.astype(DTYPE)
        batch_size = state_batch.shape[0]
        
        # -------------------------- Actorå‰å‘ä¼ æ’­ï¼ˆä¿å­˜ä¸­é—´ç»“æœï¼‰ --------------------------
        # ç¬¬ä¸€å±‚
        h1 = np.dot(state_batch, self.actor.params["w1"]) + self.actor.params["b1"]
        h1_relu = relu(h1)
        # ç¬¬äºŒå±‚
        h2 = np.dot(h1_relu, self.actor.params["w2"]) + self.actor.params["b2"]
        h2_relu = relu(h2)
        # ç¬¬ä¸‰å±‚
        h3 = np.dot(h2_relu, self.actor.params["w3"]) + self.actor.params["b3"]
        h3_relu = relu(h3)
        # è¾“å‡ºå±‚
        logits = np.dot(h3_relu, self.actor.params["w4"]) + self.actor.params["b4"]
        tanh_out = tanh(logits)
        action = self.max_action * tanh_out
        
        # -------------------------- Critic Q1å‰å‘ä¼ æ’­ --------------------------
        # è®¡ç®—Qå€¼
        q1 = self.critic.Q1(state_batch, action)
        # ActoræŸå¤±ï¼š-mean(Q1)
        loss = -np.mean(q1)
        # æŸå¤±å¯¹Q1çš„æ¢¯åº¦
        grad_q1 = -1.0 / batch_size * np.ones_like(q1, dtype=DTYPE)
        
        # -------------------------- Criticåå‘ä¼ æ’­åˆ°action --------------------------
        # æ‹¼æ¥stateå’Œaction
        x = np.concatenate([state_batch, action], axis=1)
        # Criticç½‘ç»œ1å‰å‘ï¼ˆä¿å­˜ä¸­é—´ç»“æœï¼‰
        c_h1 = np.dot(x, self.critic.params1["w1"]) + self.critic.params1["b1"]
        c_h1_relu = relu(c_h1)
        c_h2 = np.dot(c_h1_relu, self.critic.params1["w2"]) + self.critic.params1["b2"]
        c_h2_relu = relu(c_h2)
        c_h3 = np.dot(c_h2_relu, self.critic.params1["w3"]) + self.critic.params1["b3"]
        c_h3_relu = relu(c_h3)
        
        # Criticåå‘ä¼ æ’­
        # å¯¹c_h3_reluçš„æ¢¯åº¦
        grad_c_h3 = np.dot(grad_q1, self.critic.params1["w4"].T)
        grad_c_h3 = grad_c_h3 * (c_h3_relu > 0).astype(DTYPE)
        # å¯¹c_h2_reluçš„æ¢¯åº¦
        grad_c_h2 = np.dot(grad_c_h3, self.critic.params1["w3"].T)
        grad_c_h2 = grad_c_h2 * (c_h2_relu > 0).astype(DTYPE)
        # å¯¹c_h1_reluçš„æ¢¯åº¦
        grad_c_h1 = np.dot(grad_c_h2, self.critic.params1["w2"].T)
        grad_c_h1 = grad_c_h1 * (c_h1_relu > 0).astype(DTYPE)
        # å¯¹xçš„æ¢¯åº¦
        grad_x = np.dot(grad_c_h1, self.critic.params1["w1"].T)
        # æå–å¯¹actionçš„æ¢¯åº¦ï¼ˆxçš„åaction_dimåˆ—ï¼‰
        grad_action = grad_x[:, -self.action_dim:]
        
        # -------------------------- Actoråå‘ä¼ æ’­ --------------------------
        # å¯¹tanh_outçš„æ¢¯åº¦
        grad_tanh = grad_action * self.max_action
        # tanhå¯¼æ•°ï¼š1 - tanh^2
        tanh_deriv = (1 - np.square(tanh_out)).astype(DTYPE)
        grad_logits = grad_tanh * tanh_deriv
        
        # å¯¹w4/b4çš„æ¢¯åº¦
        grad_w4 = np.dot(h3_relu.T, grad_logits)
        grad_b4 = np.sum(grad_logits, axis=0)
        
        # å¯¹h3_reluçš„æ¢¯åº¦
        grad_h3 = np.dot(grad_logits, self.actor.params["w4"].T)
        grad_h3 = grad_h3 * (h3_relu > 0).astype(DTYPE)
        # å¯¹w3/b3çš„æ¢¯åº¦
        grad_w3 = np.dot(h2_relu.T, grad_h3)
        grad_b3 = np.sum(grad_h3, axis=0)
        
        # å¯¹h2_reluçš„æ¢¯åº¦
        grad_h2 = np.dot(grad_h3, self.actor.params["w3"].T)
        grad_h2 = grad_h2 * (h2_relu > 0).astype(DTYPE)
        # å¯¹w2/b2çš„æ¢¯åº¦
        grad_w2 = np.dot(h1_relu.T, grad_h2)
        grad_b2 = np.sum(grad_h2, axis=0)
        
        # å¯¹h1_reluçš„æ¢¯åº¦
        grad_h1 = np.dot(grad_h2, self.actor.params["w2"].T)
        grad_h1 = grad_h1 * (h1_relu > 0).astype(DTYPE)
        # å¯¹w1/b1çš„æ¢¯åº¦
        grad_w1 = np.dot(state_batch.T, grad_h1)
        grad_b1 = np.sum(grad_h1, axis=0)
        
        return {
            "w1": grad_w1, "b1": grad_b1,
            "w2": grad_w2, "b2": grad_b2,
            "w3": grad_w3, "b3": grad_b3,
            "w4": grad_w4, "b4": grad_b4
        }
    
    def train(self):
        """è®­ç»ƒä¸€æ­¥ï¼ˆå®Œå…¨å¯¹é½PyTorchç‰ˆæœ¬çš„è®­ç»ƒé€»è¾‘ï¼‰"""
        if len(self.replay_buffer) < self.batch_size:
            return
        
        self.total_it += 1
        
        # é‡‡æ ·æ‰¹æ¬¡æ•°æ®ï¼ˆå¯¹é½PyTorchçš„random.sampleï¼‰
        batch = random.sample(self.replay_buffer, self.batch_size)
        state_batch = np.array([e[0] for e in batch], dtype=DTYPE)
        action_batch = np.array([e[1] for e in batch], dtype=DTYPE)
        reward_batch = np.array([e[2] for e in batch], dtype=DTYPE).reshape(-1, 1)  # å¯¹é½unsqueeze(1)
        next_state_batch = np.array([e[3] for e in batch], dtype=DTYPE)
        done_batch = np.array([e[4] for e in batch], dtype=DTYPE).reshape(-1, 1)
        
        # è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆå¯¹é½PyTorchçš„with torch.no_grad()ï¼‰
        noise = np.random.normal(0, self.policy_noise, size=action_batch.shape).astype(DTYPE)
        noise = np.clip(noise, -self.noise_clip, self.noise_clip)
        next_actions = self.actor_target.forward(next_state_batch) + noise
        next_actions = np.clip(next_actions, -self.max_action, self.max_action)
        
        target_Q1, target_Q2 = self.critic_target.forward(next_state_batch, next_actions)
        target_Q = np.minimum(target_Q1, target_Q2)
        target_Q = reward_batch + (1 - done_batch) * 0.99 * target_Q
        
        # -------------------------- æ›´æ–°Critic --------------------------
        # è®¡ç®—æ¢¯åº¦
        critic_grads = self._compute_critic_grads(state_batch, action_batch, target_Q)
        # æ¢¯åº¦è£å‰ªï¼ˆå¯¹é½PyTorchï¼‰
        critic_grads = clip_grad_norm(critic_grads)
        # è·å–å½“å‰Criticå‚æ•°
        critic_flat_params = self._flatten_critic_params()
        # æ‰§è¡Œä¼˜åŒ–
        updated_critic_params = self.critic_optimizer.step(critic_flat_params, critic_grads)
        # æ›´æ–°Criticå‚æ•°
        params1, params2 = self._unflatten_critic_params(updated_critic_params)
        self.critic.set_params(params1, params2)
        
        # -------------------------- å»¶è¿Ÿæ›´æ–°Actor --------------------------
        if self.total_it % self.policy_freq == 0:
            # è®¡ç®—Actoræ¢¯åº¦
            actor_grads = self._compute_actor_grads(state_batch)
            # æ¢¯åº¦è£å‰ª
            actor_grads = clip_grad_norm(actor_grads)
            # æ‰§è¡Œä¼˜åŒ–
            updated_actor_params = self.actor_optimizer.step(self.actor.get_params(), actor_grads)
            # æ›´æ–°Actorå‚æ•°
            self.actor.set_params(updated_actor_params)
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œï¼ˆå®Œå…¨å¯¹é½PyTorchçš„copy_é€»è¾‘ï¼‰
            # æ›´æ–°Critic Target
            critic_params1, critic_params2 = self.critic.get_params()
            target_critic_params1, target_critic_params2 = self.critic_target.get_params()
            
            for k in critic_params1.keys():
                target_critic_params1[k] = 0.005 * critic_params1[k] + 0.995 * target_critic_params1[k]
            for k in critic_params2.keys():
                target_critic_params2[k] = 0.005 * critic_params2[k] + 0.995 * target_critic_params2[k]
            
            self.critic_target.set_params(target_critic_params1, target_critic_params2)
            
            # æ›´æ–°Actor Target
            actor_params = self.actor.get_params()
            target_actor_params = self.actor_target.get_params()
            
            for k in actor_params.keys():
                target_actor_params[k] = 0.005 * actor_params[k] + 0.995 * target_actor_params[k]
            
            self.actor_target.set_params(target_actor_params)
    
    def save(self, filename: str):
        """ä¿å­˜æ¨¡å‹ï¼ˆå¯¹é½PyTorchç‰ˆæœ¬çš„ä¿å­˜é€»è¾‘ï¼‰"""
        current_time = datetime.datetime.now()
        time_str = current_time.strftime("%m%d_%H%M%S")
        save_filename = f"{filename}_{time_str}.npz"
        save_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), save_filename)
        
        # æ”¶é›†æ‰€æœ‰å‚æ•°
        actor_params = self.actor.get_params()
        critic_params1, critic_params2 = self.critic.get_params()
        
        # ä¿å­˜ä¸ºnpzæ–‡ä»¶ï¼ˆå¯¹é½PyTorchçš„pthä¿å­˜ï¼‰
        np.savez(
            save_path,
            **{f"actor_{k}": v for k, v in actor_params.items()},
            **{f"critic1_{k}": v for k, v in critic_params1.items()},
            **{f"critic2_{k}": v for k, v in critic_params2.items()}
        )
        
        print(f"æœ€ä¼˜æ¨¡å‹å·²ä¿å­˜è‡³: {save_path}")

# -------------------------- è·¯å¾„é…ç½®ï¼ˆå®Œå…¨å¤ç”¨PyTorchç‰ˆæœ¬ï¼‰ --------------------------
def get_project_paths() -> Dict[str, str]:
    """
    è·å–æ‰€æœ‰æ•°æ®æ–‡ä»¶çš„è·¯å¾„ï¼ˆåŸºäºç¨‹åºæ‰€åœ¨ç›®å½•ï¼‰
    Returns:
        åŒ…å«å„ç±»æ–‡ä»¶è·¯å¾„çš„å­—å…¸
    """
    # è·å–ç¨‹åºæ‰€åœ¨ç›®å½•
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    # å®šä¹‰å„ç›®å½•è·¯å¾„
    power_data_dir = os.path.join(base_dir, "POWERdata", "C5336")
    hisdata_dir = os.path.join(power_data_dir, "train")
    modeldata_dir = os.path.join(power_data_dir, "modeldata")
    
    # æ„å»ºæ–‡ä»¶è·¯å¾„å­—å…¸
    paths = {
        "base_dir": base_dir,
        "hisdata_dir": hisdata_dir,
        "volcst_file": os.path.join(modeldata_dir, "volcst_C5336.xlsx"),
        "kvnd_file": os.path.join(modeldata_dir, "kvnd_C5336.xlsx"),
        "branch_file": os.path.join(modeldata_dir, "branch_C5336.xlsx"),
        "pv_file": os.path.join(modeldata_dir, "pv_C5336.xlsx"),
        "model_save_dir": base_dir  # æ¨¡å‹ä¿å­˜åˆ°ç¨‹åºç›®å½•
    }
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    for key, path in paths.items():
        if "dir" in key and not os.path.exists(path):
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {path}")
    
    return paths

# -------------------------- é…ç½®è¯»å–å‡½æ•°ï¼ˆå®Œå…¨å¤ç”¨PyTorchç‰ˆæœ¬ï¼‰ --------------------------
def read_voltage_limits(volcst_file: str) -> Tuple[float, float]:
    """
    è¯»å–ç”µå‹ä¸Šä¸‹é™ï¼ˆä»volcst_C5336.xlsxï¼‰
    Args:
        volcst_file: ç”µå‹é™å€¼æ–‡ä»¶è·¯å¾„
    Returns:
        (ç”µå‹ä¸‹é™, ç”µå‹ä¸Šé™)
    """
    df = pd.read_excel(volcst_file, header=None)
    # ç¬¬äºŒè¡Œç¬¬ä¸€åˆ—=ä¸‹é™ï¼Œç¬¬äºŒè¡Œç¬¬äºŒåˆ—=ä¸Šé™
    voltage_lower = float(df.iloc[1, 0])
    voltage_upper = float(df.iloc[1, 1])
    return voltage_lower, voltage_upper

def read_key_nodes(kvnd_file: str) -> List[int]:
    """
    è¯»å–å…³é”®èŠ‚ç‚¹ç´¢å¼•ï¼ˆä»kvnd_C5336.xlsxï¼‰
    Args:
        kvnd_file: å…³é”®èŠ‚ç‚¹æ–‡ä»¶è·¯å¾„
    Returns:
        å…³é”®èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨ï¼ˆèŠ‚ç‚¹å·-1è½¬æ¢ä¸ºç´¢å¼•ï¼‰
    """
    df = pd.read_excel(kvnd_file, header=0)  # ç¬¬ä¸€è¡Œæ˜¯åˆ—å
    # ä»ç¬¬äºŒè¡Œç¬¬ä¸€åˆ—å¼€å§‹è¯»å–èŠ‚ç‚¹å·ï¼Œè½¬æ¢ä¸ºç´¢å¼•ï¼ˆ-1ï¼‰
    key_node_nums = df.iloc[:, 0].dropna().astype(int).tolist()
    key_node_indices = [num - 1 for num in key_node_nums]  # èŠ‚ç‚¹å·è½¬ç´¢å¼•
    return key_node_indices

def read_branch_data(branch_file: str) -> np.ndarray:
    """
    è¯»å–æ”¯è·¯æ•°æ®ï¼ˆä»branch_C5336.xlsxï¼‰
    ã€ä¿®æ”¹ç‚¹1ã€‘è¿”å›äºŒç»´æ•°ç»„ï¼ˆæ¯è¡Œ5åˆ—ï¼šçº¿è·¯å·	é¦–èŠ‚ç‚¹	æœ«èŠ‚ç‚¹	ç”µé˜»	ç”µæŠ—ï¼‰
    Args:
        branch_file: æ”¯è·¯æ•°æ®æ–‡ä»¶è·¯å¾„
    Returns:
        äºŒç»´æ”¯è·¯æ•°æ®æ•°ç»„ (n_branches, 5)
    """
    df = pd.read_excel(branch_file, header=0)  # åˆ—åï¼šçº¿è·¯å·	é¦–èŠ‚ç‚¹	æœ«èŠ‚ç‚¹	ç”µé˜»	ç”µæŠ—
    # æŒ‰è¡Œè¯»å–ï¼Œç›´æ¥è¿”å›äºŒç»´æ•°ç»„
    branch_data = []
    for _, row in df.iterrows():
        branch_data.append([
            row.iloc[0],  # çº¿è·¯å·
            row.iloc[1],  # é¦–èŠ‚ç‚¹
            row.iloc[2],  # æœ«èŠ‚ç‚¹
            row.iloc[3],  # ç”µé˜»
            row.iloc[4]   # ç”µæŠ—
        ])
    # ã€å…³é”®ä¿®æ”¹ã€‘è¿”å›äºŒç»´æ•°ç»„ï¼Œä¸å†å±•å¹³
    return np.array(branch_data)

def read_tunable_q_nodes(pv_file: str, bus_data: np.ndarray) -> List[Tuple[int, float, float, str]]:
    """
    è¯»å–å¹¶è®¡ç®—å¯è°ƒèŠ‚æ— åŠŸèŠ‚ç‚¹é…ç½®ï¼ˆä»pv_C5336.xlsxï¼‰
    Args:
        pv_file: å…‰ä¼èŠ‚ç‚¹æ–‡ä»¶è·¯å¾„
        bus_data: BusçŸ©é˜µæ•°æ®ï¼ˆç”¨äºè·å–å½“å‰æœ‰åŠŸå€¼ï¼‰
    Returns:
        TUNABLE_Q_NODESæ ¼å¼çš„åˆ—è¡¨ï¼š[(èŠ‚ç‚¹ç´¢å¼•, æ— åŠŸä¸‹é™, æ— åŠŸä¸Šé™, èŠ‚ç‚¹åç§°), ...]
    """
    df = pd.read_excel(pv_file, header=0)  # åˆ—åï¼šèŠ‚ç‚¹å· å®¹é‡ è°ƒåº¦å‘½å
    tunable_q_nodes = []
    
    for _, row in df.iterrows():
        # è¯»å–åŸºç¡€ä¿¡æ¯
        node_num = int(row.iloc[0])  # å…‰ä¼èŠ‚ç‚¹å·
        capacity = float(row.iloc[1])  # å®¹é‡
        node_name = row.iloc[2] if not pd.isna(row.iloc[2]) else f"èŠ‚ç‚¹{node_num}"
        
        # è®¡ç®—å¯è°ƒæ— åŠŸä¸Šä¸‹é™
        node_index = node_num - 1  # è½¬ç´¢å¼•
        p_current = bus_data[node_index, 1]  # BusçŸ©é˜µä¸­è¯¥èŠ‚ç‚¹çš„æœ‰åŠŸå€¼
        q_max = np.sqrt(max(0, capacity**2 - p_current**2))  # æ— åŠŸä¸Šé™
        q_min = -q_max  # æ— åŠŸä¸‹é™
        
        # æ·»åŠ åˆ°åˆ—è¡¨
        tunable_q_nodes.append((node_index, q_min, q_max, node_name))
    
    return tunable_q_nodes

def read_training_sample(hisdata_dir: str, sample_file: str = None) -> Tuple[np.ndarray, float]:
    """
    è¯»å–è®­ç»ƒæ ·æœ¬æ•°æ®ï¼ˆä»hisdata/rltrainä¸‹çš„Excelæ–‡ä»¶ï¼‰
    Args:
        hisdata_dir: è®­ç»ƒæ ·æœ¬ç›®å½•
        sample_file: æŒ‡å®šæ ·æœ¬æ–‡ä»¶ï¼ˆNoneåˆ™å–ç¬¬ä¸€ä¸ªï¼‰
    Returns:
        (BusçŸ©é˜µæ•°æ®, åŸºå‡†ç”µå‹UB)
    """
    # è·å–æ‰€æœ‰æ ·æœ¬æ–‡ä»¶
    sample_files = glob.glob(os.path.join(hisdata_dir, "C5336_*.xlsx"))
    if not sample_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒæ ·æœ¬æ–‡ä»¶: {hisdata_dir}")
    
    # é€‰æ‹©æ ·æœ¬æ–‡ä»¶ï¼ˆæŒ‡å®šæˆ–ç¬¬ä¸€ä¸ªï¼‰
    if sample_file and os.path.exists(sample_file):
        file_path = sample_file
    else:
        file_path = sample_files[0]
    
    # è¯»å–Busæ•°æ®ï¼ˆsheet=busï¼‰
    df_bus = pd.read_excel(file_path, sheet_name="bus", header=0)  # åˆ—åï¼šèŠ‚ç‚¹å· æœ‰åŠŸå€¼ æ— åŠŸå€¼
    bus_data = []
    for _, row in df_bus.iterrows():
        bus_data.append([
            int(row.iloc[0]),    # èŠ‚ç‚¹å·
            float(row.iloc[1]),  # æœ‰åŠŸå€¼
            float(row.iloc[2])   # æ— åŠŸå€¼
        ])
    # ã€ç¡®ä¿Busæ˜¯äºŒç»´æ•°ç»„ã€‘
    bus_array = np.array(bus_data)
    
    # è¯»å–åŸºå‡†ç”µå‹UBï¼ˆsheet=slackï¼‰
    df_slack = pd.read_excel(file_path, sheet_name="slack")
    ub = float(df_slack.iloc[0, 0])  # slack sheetçš„ç¬¬ä¸€ä¸ªå€¼
    
    return bus_array, ub

# -------------------------- æ¨¡å‹ä¿å­˜ä¼˜åŒ–ï¼ˆå®Œå…¨å¤ç”¨PyTorchç‰ˆæœ¬ï¼‰ --------------------------
def save_best_normalized_model(agent, filename: str, best_normalized_improvement: float, 
                               current_normalized_improvement: float, paths: Dict[str, str]):
    """
    åŸºäºå½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦ä¿å­˜æœ€ä¼˜æ¨¡å‹
    Args:
        current_normalized_improvement: å½“å‰è½®æ¬¡çš„å¹³å‡å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦
                                        ï¼ˆæ­£æ•°è¡¨ç¤ºæ”¹å–„ï¼Œè¶Šå¤§è¶Šå¥½ï¼‰
    Returns:
        æ›´æ–°åçš„æœ€ä¼˜å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦
    """
    # å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦è¶Šå¤§è¶Šå¥½ï¼ˆè¡¨ç¤ºæ”¹å–„è¶Šå¤šï¼‰
    if current_normalized_improvement > best_normalized_improvement:
        # åˆ é™¤æ—§æ¨¡å‹
        old_models = glob.glob(os.path.join(paths["model_save_dir"], f"{filename}_*.npz"))
        for old_model in old_models:
            try:
                os.remove(old_model)
            except Exception as e:
                print(f"åˆ é™¤æ—§æ¨¡å‹å¤±è´¥: {old_model}, é”™è¯¯: {e}")
        
        # ä¿å­˜æ–°æ¨¡å‹
        agent.save(filename)
        best_normalized_improvement = current_normalized_improvement
        print(f"ğŸ“ˆ æ›´æ–°æœ€ä¼˜æ¨¡å‹ï¼Œå¹³å‡å½’ä¸€åŒ–æ”¹å–„: {best_normalized_improvement:.2f}%")
    
    return best_normalized_improvement

# -------------------------- æ”¹è¿›çš„å¼ºåŒ–å­¦ä¹ ç¯å¢ƒï¼ˆè‡ªåŠ¨è®¡ç®—åŸºå‡†ç½‘æŸç‡ï¼‰ --------------------------
class ImprovedPowerSystemEnv:
    def __init__(self, 
                 bus_data: np.ndarray,
                 branch_data: np.ndarray,
                 voltage_lower: float,
                 voltage_upper: float,
                 key_nodes: List[int],
                 tunable_q_nodes: List[Tuple[int, float, float, str]],
                 ub: float,  # ã€ä¿®æ”¹ç‚¹2ã€‘æ–°å¢UBå‚æ•°ï¼Œåˆå§‹åŒ–æ—¶ç›´æ¥ä¼ å…¥
                 baseline_info: Dict[str, float] = None,  # æ–°å¢ï¼šåŸºå‡†ä¿¡æ¯
                 sb: float = 10.0,
                 pr: float = 1e-6):
        """
        åˆå§‹åŒ–ç”µåŠ›ç³»ç»Ÿç¯å¢ƒï¼ˆæ”¹è¿›ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºå‡†ç½‘æŸç‡ï¼‰
        Args:
            bus_data: BusçŸ©é˜µæ•°æ®
            branch_data: æ”¯è·¯æ•°æ®ï¼ˆäºŒç»´æ•°ç»„ï¼‰
            voltage_lower: ç”µå‹ä¸‹é™
            voltage_upper: ç”µå‹ä¸Šé™
            key_nodes: å…³é”®èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨
            tunable_q_nodes: å¯è°ƒèŠ‚æ— åŠŸèŠ‚ç‚¹é…ç½®
            ub: åŸºå‡†ç”µå‹ï¼ˆä»è®­ç»ƒæ ·æœ¬è¯»å–ï¼‰
            baseline_info: åŸºå‡†ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«baseline_lossï¼ˆåŸºå‡†ç½‘æŸç‡ï¼‰å’Œtotal_powerï¼ˆæ€»åŠŸç‡ï¼‰
            sb: åŸºå‡†åŠŸç‡ MVA
            pr: æ½®æµæ”¶æ•›ç²¾åº¦
        """
        self.Bus = bus_data
        self.Branch = branch_data  # ç°åœ¨æ˜¯äºŒç»´æ•°ç»„ (n_branches, 5)
        self.SB = sb
        self.UB = ub  # ã€å…³é”®ä¿®æ”¹ã€‘åˆå§‹åŒ–æ—¶ç›´æ¥èµ‹å€¼ï¼Œé¿å…None
        self.pr = pr
        
        self.state_dim = len(key_nodes)
        self.action_dim = len(tunable_q_nodes)
        self.max_action = 1.0
        
        # æ— åŠŸä¸Šä¸‹é™
        self.q_mins = np.array([node[1] for node in tunable_q_nodes])
        self.q_maxs = np.array([node[2] for node in tunable_q_nodes])
        self.tunable_q_nodes = tunable_q_nodes
        
        self.v_min = voltage_lower
        self.v_max = voltage_upper
        self.key_nodes = key_nodes
        
        self.previous_loss_rate = None
        self.previous_voltages = None
        self.previous_actions = None
        
        # åŸºå‡†ä¿¡æ¯ï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼Œè‡ªåŠ¨è®¡ç®—ï¼‰
        if baseline_info is None:
            self.baseline_info = self._calculate_baseline_info()
        else:
            self.baseline_info = baseline_info
        
        # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
        self._calculate_normalization_params()
        
        self.reset()  # ç°åœ¨UBå·²åˆå§‹åŒ–ï¼Œresetä¸ä¼šæŠ¥é”™
    
    def _calculate_baseline_info(self) -> Dict[str, float]:
        """è‡ªåŠ¨è®¡ç®—åŸºå‡†ç½‘æŸç‡å’Œæ€»åŠŸç‡"""
        # ä½¿ç”¨åˆå§‹æ— åŠŸå€¼è®¡ç®—æ½®æµï¼ˆå³ä¸è°ƒèŠ‚ï¼‰
        initial_q = [self.Bus[node[0], 2] for node in self.tunable_q_nodes]
        loss_rate, voltages, power_info = self._power_flow(initial_q)
        
        if loss_rate is None or power_info is None:
            # å¦‚æœåˆå§‹æ½®æµä¸æ”¶æ•›ï¼Œä¼°ç®—åŸºå‡†å€¼
            total_input_power = self._estimate_total_power()
            baseline_loss = 10.0  # é»˜è®¤åŸºå‡†ç½‘æŸç‡
        else:
            total_input_power = power_info[2]  # æ€»è¾“å…¥åŠŸç‡
            baseline_loss = loss_rate
        
        return {
            'baseline_loss': baseline_loss,
            'total_power': total_input_power
        }
    
    def _estimate_total_power(self) -> float:
        """ä¼°ç®—æ€»è¾“å…¥åŠŸç‡ï¼ˆå¦‚æœæ½®æµè®¡ç®—å¤±è´¥æ—¶ä½¿ç”¨ï¼‰"""
        # ç®€å•ä¼°ç®—ï¼šæ‰€æœ‰è´Ÿè·èŠ‚ç‚¹çš„æœ‰åŠŸç»å¯¹å€¼ä¹‹å’Œ * 1.1
        load_power = np.sum(np.abs(self.Bus[self.Bus[:, 1] > 0, 1]))  # è´Ÿè·èŠ‚ç‚¹æœ‰åŠŸ
        return load_power * 1.1  # ä¹˜ä»¥ç³»æ•°è€ƒè™‘ç½‘æŸ
    
    def _calculate_normalization_params(self):
        """åŸºäºåŸºå‡†ä¿¡æ¯è®¡ç®—å½’ä¸€åŒ–å‚æ•°"""
        baseline_loss = self.baseline_info['baseline_loss']
        total_power = self.baseline_info['total_power']
        
        # 1. åŸºäºåŠŸç‡çš„æƒé‡ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰
        # é«˜åŠŸç‡æ ·æœ¬æƒé‡å¤§ï¼Œä½åŠŸç‡æ ·æœ¬æƒé‡å°
        self.power_weight = np.clip(total_power / 100.0, 0.5, 2.0)
        
        # 2. åŸºäºåŸºå‡†ç½‘æŸç‡çš„éš¾åº¦ç³»æ•°
        # åŸºå‡†ç½‘æŸç‡é«˜çš„æ ·æœ¬ï¼Œä¼˜åŒ–ç©ºé—´å¤§ï¼Œéš¾åº¦ä½
        # åŸºå‡†ç½‘æŸç‡ä½çš„æ ·æœ¬ï¼Œä¼˜åŒ–ç©ºé—´å°ï¼Œéš¾åº¦é«˜
        if baseline_loss > 8.0:
            self.difficulty_factor = 0.8  # å®¹æ˜“
        elif baseline_loss > 5.0:
            self.difficulty_factor = 1.0  # ä¸­ç­‰
        elif baseline_loss > 3.0:
            self.difficulty_factor = 1.2  # è¾ƒéš¾
        else:
            self.difficulty_factor = 1.5  # å¾ˆéš¾
    
    def reset(self):
        """é‡ç½®ç¯å¢ƒï¼Œè®¡ç®—åˆå§‹çŠ¶æ€"""
        # ä»åˆå§‹æ½®æµè®¡ç®—è·å–åˆå§‹ç”µå‹
        initial_q = [self.Bus[node[0], 2] for node in self.tunable_q_nodes]
        _, initial_voltages, power_info = self._power_flow(initial_q)
        
        if initial_voltages is None:
            initial_voltages = np.ones(self.Bus.shape[0]) * self.UB
        
        self.state = self._build_state(initial_voltages)
        self.previous_loss_rate = None
        self.previous_voltages = None
        self.previous_actions = None
        
        return self.state
    
    def _build_state(self, voltages):
        """æ„å»ºå½’ä¸€åŒ–çš„çŠ¶æ€å‘é‡"""
        key_voltages = voltages[self.key_nodes]
        normalized_voltages = (key_voltages - self.v_min) / (self.v_max - self.v_min) * 2 - 1
        return normalized_voltages
    
    def step(self, action):
        """æ‰§è¡ŒåŠ¨ä½œï¼Œè¿”å›æ–°çŠ¶æ€å’Œå¥–åŠ±"""
        actual_action = self._denormalize_action(action)
        loss_rate, voltages, power_info = self._power_flow(actual_action)
        
        if loss_rate is None:
            reward = -200  # æ½®æµä¸æ”¶æ•›æƒ©ç½šåŠ é‡
            done = True
            next_state = self.state
            info = {
                "loss_rate": 100, 
                "voltages": np.ones(self.Bus.shape[0]) * self.UB,
                "normalized_improvement": -100,
                "relative_improvement": -100
            }
        else:
            # è®¡ç®—å½’ä¸€åŒ–å¥–åŠ±
            reward, normalized_improvement, relative_improvement = self._calculate_normalized_reward(
                loss_rate, voltages, actual_action
            )
            
            done = False
            next_state = self._build_state(voltages)
            self.state = next_state
            self.previous_loss_rate = loss_rate
            self.previous_voltages = voltages.copy()
            self.previous_actions = actual_action
            
            info = {
                "loss_rate": loss_rate,
                "voltages": voltages,
                "normalized_improvement": normalized_improvement,
                "relative_improvement": relative_improvement,
                "baseline_loss": self.baseline_info['baseline_loss']
            }
        
        return next_state, reward, done, info
    
    def _denormalize_action(self, action):
        """åå½’ä¸€åŒ–åŠ¨ä½œåˆ°å®é™…æ— åŠŸå€¼èŒƒå›´"""
        actual_actions = []
        for i in range(len(action)):
            normalized = np.clip(action[i], -1, 1)
            q_min = self.q_mins[i]
            q_max = self.q_maxs[i]
            actual = (normalized + 1) / 2 * (q_max - q_min) + q_min
            actual_actions.append(actual)
        return actual_actions
    
    def _calculate_normalized_reward(self, loss_rate: float, voltages: np.ndarray, 
                                    actual_action: List[float]) -> Tuple[float, float, float]:
        """
        æ ¸å¿ƒï¼šè‡ªåŠ¨å½’ä¸€åŒ–çš„å¥–åŠ±è®¡ç®—
        è¿”å›ï¼š(æ€»å¥–åŠ±, å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦, ç›¸å¯¹æ”¹å–„ç™¾åˆ†æ¯”)
        """
        baseline_loss = self.baseline_info['baseline_loss']
        
        # 1. è®¡ç®—ç›¸å¯¹æ”¹å–„ç™¾åˆ†æ¯”ï¼ˆæ ¸å¿ƒå½’ä¸€åŒ–æŒ‡æ ‡ï¼‰
        if baseline_loss > 0:
            relative_improvement = (baseline_loss - loss_rate) / baseline_loss * 100
        else:
            relative_improvement = 0
        
        # 2. è®¡ç®—å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦ï¼ˆåº”ç”¨æƒé‡å’Œéš¾åº¦ç³»æ•°ï¼‰
        normalized_improvement = relative_improvement * self.power_weight * self.difficulty_factor
        
        # 3. è®¡ç®—åŸºç¡€å¥–åŠ±ï¼ˆåŸºäºå½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦ï¼‰
        base_reward = normalized_improvement * 2.0  # ç¼©æ”¾ç³»æ•°
        
        # 4. ç”µå‹çº¦æŸæƒ©ç½šï¼ˆç¡¬çº¦æŸï¼‰
        voltage_penalty = 0
        voltage_reward = 0
        for v in voltages:
            if v < self.v_min:
                voltage_penalty += 100 * (self.v_min - v)  # ç”µå‹è¶Šé™ä¸¥é‡æƒ©ç½š
            elif v > self.v_max:
                voltage_penalty += 100 * (v - self.v_max)
            else:
                # ç”µå‹åœ¨æ­£å¸¸èŒƒå›´å†…ï¼Œç»™äºˆå¥–åŠ±ï¼Œé¼“åŠ±æ¥è¿‘æœ€ä¼˜
                optimal_voltage = (self.v_min + self.v_max) / 2
                distance = abs(v - optimal_voltage) / optimal_voltage
                voltage_reward += 5 * (1 - distance)  # è¶Šæ¥è¿‘æœ€ä¼˜ï¼Œå¥–åŠ±è¶Šé«˜
        
        # 5. åŠ¨ä½œå¹³æ»‘æ€§å¥–åŠ±ï¼ˆé¼“åŠ±å°å¹…è°ƒèŠ‚ï¼‰
        action_penalty = 0
        if self.previous_actions is not None:
            # è®¡ç®—åŠ¨ä½œå˜åŒ–é‡
            action_change = np.sum(np.abs(np.array(actual_action) - np.array(self.previous_actions)))
            # é¼“åŠ±å¹³æ»‘è°ƒèŠ‚ï¼Œå¤§å¹…å˜åŒ–æœ‰æƒ©ç½š
            if action_change > self.max_action * 0.5:  # å˜åŒ–è¶…è¿‡50%
                action_penalty = 5 * (action_change - self.max_action * 0.5)
        
        # 6. æ”¶æ•›æ€§å¥–åŠ±ï¼ˆé¼“åŠ±å¿«é€Ÿæ”¹å–„ï¼‰
        convergence_bonus = 0
        if self.previous_loss_rate is not None:
            # è®¡ç®—æ”¹å–„é€Ÿåº¦
            improvement_speed = (self.previous_loss_rate - loss_rate) / max(self.previous_loss_rate, 1.0)
            if improvement_speed > 0.1:  # æ”¹å–„è¶…è¿‡10%
                convergence_bonus = 10 * improvement_speed
        
        # 7. ç»¼åˆå¥–åŠ±
        total_reward = (
            base_reward + 
            voltage_reward + 
            convergence_bonus - 
            voltage_penalty - 
            action_penalty
        )
        
        # 8. ç‰¹åˆ«å¥–åŠ±ï¼šå¦‚æœæ»¡è¶³æ‰€æœ‰çº¦æŸä¸”æœ‰æ˜¾è‘—æ”¹å–„
        if (voltage_penalty == 0 and 
            relative_improvement > 10 and  # ç›¸å¯¹æ”¹å–„è¶…è¿‡10%
            action_penalty < 2):  # åŠ¨ä½œå¹³æ»‘
            total_reward += 20
        
        return total_reward, normalized_improvement, relative_improvement
    
    def _power_flow(self, tunable_q_values):
        """å†…éƒ¨æ½®æµè®¡ç®—å‡½æ•°ï¼ˆé€‚é…é€šç”¨åŒ–å‚æ•°ï¼‰"""
        Bus_copy = copy.deepcopy(self.Bus)
        Branch_copy = copy.deepcopy(self.Branch)  # ç°åœ¨æ˜¯äºŒç»´æ•°ç»„
        
        # æ›´æ–°å¯è°ƒæ— åŠŸèŠ‚ç‚¹å€¼
        for i, (node_idx, _, _, _) in enumerate(self.tunable_q_nodes):
            Bus_copy[node_idx, 2] = tunable_q_values[i]
        
        # æ ‡å¹ºå€¼è½¬æ¢ï¼ˆç°åœ¨Branch_copyæ˜¯äºŒç»´æ•°ç»„ï¼Œç´¢å¼•æ­£å¸¸ï¼‰
        Bus_copy[:, 1] = Bus_copy[:, 1] / self.SB
        Bus_copy[:, 2] = Bus_copy[:, 2] / self.SB
        Branch_copy[:, 3] = Branch_copy[:, 3] * self.SB / (self.UB **2)  # ç”µé˜»æ ‡å¹ºå€¼
        Branch_copy[:, 4] = Branch_copy[:, 4] * self.SB / (self.UB** 2)  # ç”µæŠ—æ ‡å¹ºå€¼
        
        busnum = Bus_copy.shape[0]
        branchnum = Branch_copy.shape[0]
        
        # èŠ‚ç‚¹ç±»å‹åˆ¤æ–­
        node_types = []
        for i in range(busnum):
            node_id = Bus_copy[i, 0]
            p = Bus_copy[i, 1]
            if node_id == 1:
                node_types.append("å¹³è¡¡èŠ‚ç‚¹")
            elif p < 0:
                node_types.append("å…‰ä¼èŠ‚ç‚¹")
            elif p > 0:
                node_types.append("è´Ÿè·èŠ‚ç‚¹")
            else:
                node_types.append("æ™®é€šèŠ‚ç‚¹")
        
        # åˆå§‹åŒ–ç”µå‹å’Œç›¸è§’
        Vbus = np.ones(busnum)
        Vbus[0] = 1.0
        cita = np.zeros(busnum)
        
        k = 0
        Ploss = np.zeros(branchnum)
        Qloss = np.zeros(branchnum)
        P = np.zeros(branchnum)
        Q = np.zeros(branchnum)
        F = 0
        
        # æ”¯è·¯æ’åºï¼ˆé€‚é…äºŒç»´Branchæ•°æ®ï¼‰
        TempBranch = Branch_copy.copy()
        s1 = np.zeros((0, 5))
        while TempBranch.size > 0:
            s = TempBranch.shape[0] - 1
            s2 = np.zeros((0, 5))
            while s >= 0:
                # æŸ¥æ‰¾ä»¥å½“å‰æ”¯è·¯æœ«èŠ‚ç‚¹ä¸ºé¦–èŠ‚ç‚¹çš„æ”¯è·¯
                i = np.where(TempBranch[:, 1] == TempBranch[s, 2])[0]
                if i.size == 0:
                    # æ²¡æœ‰åç»­æ”¯è·¯ï¼ŒåŠ å…¥s1
                    if s1.size == 0:
                        s1 = TempBranch[s, :].reshape(1, -1)
                    else:
                        s1 = np.vstack([s1, TempBranch[s, :]])
                else:
                    # æœ‰åç»­æ”¯è·¯ï¼ŒåŠ å…¥s2
                    if s2.size == 0:
                        s2 = TempBranch[s, :].reshape(1, -1)
                    else:
                        s2 = np.vstack([s2, TempBranch[s, :]])
                s -= 1
            TempBranch = s2.copy()
        
        # æ½®æµè¿­ä»£è®¡ç®—
        while k < 100 and F == 0:
            Pij1 = np.zeros(busnum)
            Qij1 = np.zeros(busnum)
            
            for s in range(branchnum):
                ii = int(s1[s, 1] - 1)  # é¦–èŠ‚ç‚¹ç´¢å¼•
                jj = int(s1[s, 2] - 1)  # æœ«èŠ‚ç‚¹ç´¢å¼•
                Pload = Bus_copy[jj, 1]
                Qload = Bus_copy[jj, 2]
                R = s1[s, 3]
                X = s1[s, 4]
                VV = Vbus[jj]
                
                Pij0 = Pij1[jj]
                Qij0 = Qij1[jj]
                
                II = ((Pload + Pij0)**2 + (Qload + Qij0)** 2) / (VV**2)
                Ploss[s] = II * R
                Qloss[s] = II * X
                
                P[s] = Pload + Ploss[s] + Pij0
                Q[s] = Qload + Qloss[s] + Qij0
                
                Pij1[ii] += P[s]
                Qij1[ii] += Q[s]
            
            # ç”µå‹è®¡ç®—
            for s in range(branchnum-1, -1, -1):
                ii = int(s1[s, 2] - 1)  # æœ«èŠ‚ç‚¹ç´¢å¼•
                kk = int(s1[s, 1] - 1)  # é¦–èŠ‚ç‚¹ç´¢å¼•
                R = s1[s, 3]
                X = s1[s, 4]
                
                V_real = Vbus[kk] - (P[s]*R + Q[s]*X) / Vbus[kk]
                V_imag = (P[s]*X - Q[s]*R) / Vbus[kk]
                
                Vbus[ii] = np.sqrt(V_real**2 + V_imag**2)
                cita[ii] = cita[kk] - np.arctan2(V_imag, V_real)
            
            # æ ¡éªŒæ”¶æ•›
            Pij2 = np.zeros(busnum)
            Qij2 = np.zeros(busnum)
            for s in range(branchnum):
                ii = int(s1[s, 1] - 1)
                jj = int(s1[s, 2] - 1)
                Pload = Bus_copy[jj, 1]
                Qload = Bus_copy[jj, 2]
                R = s1[s, 3]
                X = s1[s, 4]
                VV = Vbus[jj]
                
                Pij0 = Pij2[jj]
                Qij0 = Qij2[jj]
                
                II = ((Pload + Pij0)**2 + (Qload + Qij0)** 2) / (VV**2)
                P_val = Pload + II * R + Pij0
                Q_val = Qload + II * X + Qij0
                
                Pij2[ii] += P_val
                Qij2[ii] += Q_val
            
            ddp = np.max(np.abs(Pij1 - Pij2))
            ddq = np.max(np.abs(Qij1 - Qij2))
            if ddp < self.pr and ddq < self.pr:
                F = 1
            k += 1
        
        # æ”¶æ•›åˆ¤æ–­
        if k == 100:
            return None, None, None
        
        # è®¡ç®—ç½‘æŸç‡
        P1 = np.sum(Ploss)
        balance_node_output = Pij2[0] * self.SB
        pv_nodes_mask = [typ == "å…‰ä¼èŠ‚ç‚¹" for typ in node_types]
        pv_total_injection = sum(-Bus_copy[i, 1] for i in range(busnum) if pv_nodes_mask[i]) * self.SB
        total_input_power = balance_node_output + pv_total_injection
        
        load_nodes_mask = [typ == "è´Ÿè·èŠ‚ç‚¹" for typ in node_types]
        total_output_power = sum(Bus_copy[i, 1] for i in range(busnum) if load_nodes_mask[i]) * self.SB
        
        loss_rate = (total_input_power - total_output_power) / total_input_power * 100 if total_input_power != 0 else 0.0
        Vbus_kv = Vbus * self.UB
        
        return loss_rate, Vbus_kv, (balance_node_output, pv_total_injection, total_input_power, total_output_power)

# -------------------------- æ”¹è¿›çš„æ ·æœ¬ç®¡ç†å™¨ï¼ˆè‡ªåŠ¨è®¡ç®—åŸºå‡†ç½‘æŸç‡ï¼‰ --------------------------
class ImprovedSampleManager:
    """æ”¹è¿›çš„æ ·æœ¬ç®¡ç†å™¨ï¼Œè‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŸºå‡†ç½‘æŸç‡"""
    def __init__(self, paths: Dict[str, str]):
        """
        åˆå§‹åŒ–æ ·æœ¬ç®¡ç†å™¨
        Args:
            paths: è·¯å¾„å­—å…¸
        """
        self.paths = paths
        self.hisdata_dir = paths["hisdata_dir"]
        
        # è·å–æ‰€æœ‰æ ·æœ¬æ–‡ä»¶
        self.sample_files = glob.glob(os.path.join(self.hisdata_dir, "C5336_*.xlsx"))
        if not self.sample_files:
            raise FileNotFoundError(f"æœªæ‰¾åˆ°è®­ç»ƒæ ·æœ¬æ–‡ä»¶: {self.hisdata_dir}")
        
        print(f"æ‰¾åˆ° {len(self.sample_files)} ä¸ªè®­ç»ƒæ ·æœ¬")
        
        # è‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŸºå‡†ç½‘æŸç‡
        self.sample_baselines = self._calculate_all_baselines()
        
        # è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡é‡
        self._calculate_normalization_statistics()
    
    def get_sample_data(self, sample_file: str) -> Tuple[np.ndarray, float, List[Tuple[int, float, float, str]]]:
        """
        è·å–æŒ‡å®šæ ·æœ¬çš„æ•°æ®
        Args:
            sample_file: æ ·æœ¬æ–‡ä»¶è·¯å¾„
        Returns:
            (bus_data, ub, tunable_q_nodes)
        """
        # è¯»å–Busæ•°æ®å’ŒUB
        bus_data, ub = read_training_sample(self.paths["hisdata_dir"], sample_file)
        
        # è¯»å–å¹¶è®¡ç®—å¯è°ƒæ— åŠŸèŠ‚ç‚¹
        tunable_q_nodes = read_tunable_q_nodes(self.paths["pv_file"], bus_data)
        
        return bus_data, ub, tunable_q_nodes
    
    def _calculate_baseline_for_sample(self, sample_file: str) -> Dict[str, float]:
        """
        è®¡ç®—å•ä¸ªæ ·æœ¬çš„åŸºå‡†ä¿¡æ¯
        Returns: åŒ…å«baseline_losså’Œtotal_powerçš„å­—å…¸
        """
        # è¯»å–æ ·æœ¬æ•°æ®
        bus_data, ub = read_training_sample(self.paths["hisdata_dir"], sample_file)
        tunable_q_nodes = read_tunable_q_nodes(self.paths["pv_file"], bus_data)
        
        # è¯»å–åŸºç¡€é…ç½®ï¼ˆä¸´æ—¶ï¼‰
        voltage_lower, voltage_upper = read_voltage_limits(self.paths["volcst_file"])
        key_nodes = read_key_nodes(self.paths["kvnd_file"])
        branch_data = read_branch_data(self.paths["branch_file"])
        
        # åˆ›å»ºä¸´æ—¶ç¯å¢ƒè®¡ç®—åŸºå‡†
        temp_env = ImprovedPowerSystemEnv(
            bus_data=bus_data,
            branch_data=branch_data,
            voltage_lower=voltage_lower,
            voltage_upper=voltage_upper,
            key_nodes=key_nodes,
            tunable_q_nodes=tunable_q_nodes,
            ub=ub
        )
        
        return temp_env.baseline_info
    
    def _calculate_all_baselines(self) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŸºå‡†ä¿¡æ¯"""
        baselines = {}
        
        print("è‡ªåŠ¨è®¡ç®—æ‰€æœ‰æ ·æœ¬çš„åŸºå‡†ç½‘æŸç‡...")
        for i, sample_file in enumerate(self.sample_files):
            sample_name = os.path.basename(sample_file)
            try:
                baseline_info = self._calculate_baseline_for_sample(sample_file)
                baselines[sample_file] = {
                    'name': sample_name,
                    'baseline_loss': baseline_info['baseline_loss'],
                    'total_power': baseline_info['total_power']
                }
                print(f"  æ ·æœ¬ {i+1}/{len(self.sample_files)} ({sample_name}): "
                      f"åŸºå‡†ç½‘æŸç‡={baseline_info['baseline_loss']:.4f}%, "
                      f"æ€»åŠŸç‡={baseline_info['total_power']:.2f}MW")
            except Exception as e:
                print(f"  æ ·æœ¬ {sample_name} åŸºå‡†è®¡ç®—å¤±è´¥: {e}")
                # ä½¿ç”¨é»˜è®¤å€¼
                baselines[sample_file] = {
                    'name': sample_name,
                    'baseline_loss': 10.0,
                    'total_power': 100.0
                }
        
        return baselines
    
    def _calculate_normalization_statistics(self):
        """è®¡ç®—åŸºå‡†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.sample_baselines:
            return
        
        baseline_values = [v['baseline_loss'] for v in self.sample_baselines.values()]
        power_values = [v['total_power'] for v in self.sample_baselines.values()]
        
        print(f"\nåŸºå‡†ç½‘æŸç‡ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {min(baseline_values):.4f}%")
        print(f"  æœ€å¤§å€¼: {max(baseline_values):.4f}%")
        print(f"  å¹³å‡å€¼: {np.mean(baseline_values):.4f}%")
        print(f"  ä¸­ä½æ•°: {np.median(baseline_values):.4f}%")
        print(f"  æ ‡å‡†å·®: {np.std(baseline_values):.4f}%")
        
        print(f"\næ€»è¾“å…¥åŠŸç‡ç»Ÿè®¡:")
        print(f"  æœ€å°å€¼: {min(power_values):.2f}MW")
        print(f"  æœ€å¤§å€¼: {max(power_values):.2f}MW")
        print(f"  å¹³å‡å€¼: {np.mean(power_values):.2f}MW")
        print(f"  ä¸­ä½æ•°: {np.median(power_values):.2f}MW")
    
    def create_env_for_sample(self, 
                             sample_file: str,
                             branch_data: np.ndarray,
                             voltage_lower: float,
                             voltage_upper: float,
                             key_nodes: List[int]) -> ImprovedPowerSystemEnv:
        """
        ä¸ºæŒ‡å®šæ ·æœ¬åˆ›å»ºç¯å¢ƒï¼ŒåŒ…å«è‡ªåŠ¨è®¡ç®—çš„åŸºå‡†ä¿¡æ¯
        """
        bus_data, ub, tunable_q_nodes = self.get_sample_data(sample_file)
        
        # è·å–è¯¥æ ·æœ¬çš„åŸºå‡†ä¿¡æ¯
        if sample_file in self.sample_baselines:
            baseline_info = {
                'baseline_loss': self.sample_baselines[sample_file]['baseline_loss'],
                'total_power': self.sample_baselines[sample_file]['total_power']
            }
        else:
            # å¦‚æœæ²¡æœ‰é¢„å…ˆè®¡ç®—ï¼Œç°åœºè®¡ç®—
            baseline_info = self._calculate_baseline_for_sample(sample_file)
        
        env = ImprovedPowerSystemEnv(
            bus_data=bus_data,
            branch_data=branch_data,
            voltage_lower=voltage_lower,
            voltage_upper=voltage_upper,
            key_nodes=key_nodes,
            tunable_q_nodes=tunable_q_nodes,
            ub=ub,
            baseline_info=baseline_info  # ä¼ å…¥åŸºå‡†ä¿¡æ¯
        )
        
        return env
    
    def get_all_sample_files(self) -> List[str]:
        """è·å–æ‰€æœ‰æ ·æœ¬æ–‡ä»¶è·¯å¾„"""
        return self.sample_files
    
    def get_baseline_stats(self) -> Dict[str, float]:
        """è·å–åŸºå‡†ç»Ÿè®¡ä¿¡æ¯"""
        if not self.sample_baselines:
            return {}
        
        baseline_values = [v['baseline_loss'] for v in self.sample_baselines.values()]
        
        return {
            'min': min(baseline_values),
            'max': max(baseline_values),
            'mean': np.mean(baseline_values),
            'median': np.median(baseline_values),
            'std': np.std(baseline_values)
        }

# -------------------------- æ”¹è¿›çš„åˆ†å±‚è®­ç»ƒå‡½æ•°ï¼ˆåŸºäºå½’ä¸€åŒ–æ”¹å–„ï¼‰ --------------------------
def train_rl_model_with_normalized_improvement():
    """åŸºäºå½’ä¸€åŒ–æ”¹å–„çš„å¤šæ ·æœ¬åˆ†å±‚è®­ç»ƒ"""
    # -------------------------- 1. è¯»å–æ‰€æœ‰é…ç½®å’Œæ•°æ® --------------------------
    print("=== è¯»å–é…ç½®æ–‡ä»¶ ===")
    paths = get_project_paths()
    
    # è¯»å–åŸºç¡€é…ç½®
    voltage_lower, voltage_upper = read_voltage_limits(paths["volcst_file"])
    key_nodes = read_key_nodes(paths["kvnd_file"])
    branch_data = read_branch_data(paths["branch_file"])  # ç°åœ¨æ˜¯äºŒç»´æ•°ç»„
    
    # åˆå§‹åŒ–æ”¹è¿›çš„æ ·æœ¬ç®¡ç†å™¨ï¼ˆè‡ªåŠ¨è®¡ç®—åŸºå‡†ç½‘æŸç‡ï¼‰
    sample_manager = ImprovedSampleManager(paths)
    sample_files = sample_manager.get_all_sample_files()
    
    # è·å–åŸºå‡†ç»Ÿè®¡ä¿¡æ¯
    baseline_stats = sample_manager.get_baseline_stats()
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    print(f"ç”µå‹ä¸Šä¸‹é™: [{voltage_lower:.2f}kV, {voltage_upper:.2f}kV]")
    print(f"å…³é”®èŠ‚ç‚¹ç´¢å¼•: {key_nodes} (å…±{len(key_nodes)}ä¸ª)")
    print(f"åŸºå‡†ç”µå‹UB: æ ¹æ®æ ·æœ¬å˜åŒ–")
    print(f"æ”¯è·¯æ•°: {branch_data.shape[0]}")
    print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(sample_files)}")
    print(f"åŸºå‡†ç½‘æŸç‡ä¸­ä½æ•°: {baseline_stats.get('median', 0):.4f}%")
    
    # -------------------------- 2. åˆå§‹åŒ–æ™ºèƒ½ä½“ --------------------------
    print("\n=== åˆå§‹åŒ–æ™ºèƒ½ä½“ ===")
    
    # å…ˆç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬è·å–çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
    first_bus_data, first_ub, first_tunable_q_nodes = sample_manager.get_sample_data(sample_files[0])
    
    state_dim = len(key_nodes)
    action_dim = len(first_tunable_q_nodes)
    max_action = 1.0
    
    agent = TD3(
        state_dim=state_dim,
        action_dim=action_dim,
        max_action=max_action
    )
    
    # -------------------------- 3. è®­ç»ƒå‚æ•°é…ç½® --------------------------
    # åˆ†å±‚è®­ç»ƒå‚æ•°
    samples_per_epoch = 10  # æ¯ä¸ªæ ·æœ¬è¿ç»­è®­ç»ƒçš„episodeæ•°
    epochs = 300  # è®­ç»ƒè½®æ•°ï¼ˆæ¯ä¸ªæ ·æœ¬è®­ç»ƒsamples_per_epochä¸ªepisodeï¼‰
    
    max_steps = 3
    rewards_history = []
    normalized_improvements_history = []
    loss_rates_history = []
    best_normalized_improvement = -float('inf')  # å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦è¶Šå¤§è¶Šå¥½
    
    print("\n=== å¼€å§‹åŸºäºå½’ä¸€åŒ–æ”¹å–„çš„åˆ†å±‚è®­ç»ƒ ===")
    print(f"çŠ¶æ€ç»´åº¦: {state_dim}, åŠ¨ä½œç»´åº¦: {action_dim}")
    print(f"è®­ç»ƒè½®æ•°: {epochs}, æ¯æ ·æœ¬è¿ç»­è®­ç»ƒ: {samples_per_epoch}ä¸ªepisode")
    print(f"æ¯episodeæœ€å¤§æ­¥æ•°: {max_steps}")
    print(f"è®­ç»ƒç­–ç•¥: æ¯ä¸ªæ ·æœ¬è¿ç»­è®­ç»ƒ{samples_per_epoch}ä¸ªepisodeååˆ‡æ¢åˆ°ä¸‹ä¸€ä¸ªæ ·æœ¬")
    print(f"å¥–åŠ±æœºåˆ¶: åŸºäºç›¸å¯¹æ”¹å–„ç™¾åˆ†æ¯”ï¼Œè‡ªåŠ¨å½’ä¸€åŒ–")
    
    # -------------------------- 4. é¢„å¡«å……ç»éªŒæ±  --------------------------
    print("é¢„å¡«å……ç»éªŒæ± ...")
    prefill_steps = 0
    for sample_file in sample_files:
        if len(agent.replay_buffer) >= 5000:
            break
            
        env = sample_manager.create_env_for_sample(
            sample_file, branch_data, voltage_lower, voltage_upper, key_nodes
        )
        
        state = env.reset()
        for step in range(max_steps):
            action = np.random.uniform(-1, 1, size=action_dim)
            next_state, reward, done, info = env.step(action)
            agent.replay_buffer.append((state, action, reward, next_state, done))
            prefill_steps += 1
            
            if done or len(agent.replay_buffer) >= 5000:
                break
            state = next_state
    
    print(f"é¢„å¡«å……å®Œæˆï¼Œç»éªŒæ± å¤§å°: {len(agent.replay_buffer)}")
    
    # -------------------------- 5. åˆ†å±‚è®­ç»ƒå¾ªç¯ --------------------------
    for epoch in range(epochs):
        epoch_rewards = []
        epoch_normalized_improvements = []
        epoch_loss_rates = []
        
        # éå†æ‰€æœ‰æ ·æœ¬
        for sample_idx, sample_file in enumerate(sample_files):
            sample_name = os.path.basename(sample_file)
            
            # ä¸ºå½“å‰æ ·æœ¬åˆ›å»ºç¯å¢ƒ
            env = sample_manager.create_env_for_sample(
                sample_file, branch_data, voltage_lower, voltage_upper, key_nodes
            )
            
            # åœ¨å½“å‰æ ·æœ¬ä¸Šè¿ç»­è®­ç»ƒsamples_per_epochä¸ªepisode
            for episode_in_sample in range(samples_per_epoch):
                state = env.reset()
                episode_reward = 0
                episode_normalized_improvement = 0
                episode_loss_rate = 0
                
                for step in range(max_steps):
                    # é€’å‡æ¢ç´¢å™ªå£°ï¼ˆåŸºäºæ€»è®­ç»ƒè¿›åº¦ï¼‰
                    total_episodes = epoch * len(sample_files) * samples_per_epoch + \
                                   sample_idx * samples_per_epoch + episode_in_sample
                    max_total_episodes = epochs * len(sample_files) * samples_per_epoch
                    exploration_noise = max(0.05, 0.3 * (1 - total_episodes / max_total_episodes))
                    
                    action = agent.select_action(state, exploration_noise)
                    next_state, reward, done, info = env.step(action)
                    
                    # å­˜å‚¨ç»éªŒ
                    agent.replay_buffer.append((state, action, reward, next_state, done))
                    
                    # è®­ç»ƒæ™ºèƒ½ä½“
                    if len(agent.replay_buffer) > agent.batch_size:
                        agent.train()
                    
                    # æ›´æ–°çŠ¶æ€å’Œå¥–åŠ±
                    state = next_state
                    episode_reward += reward
                    episode_normalized_improvement = info.get('normalized_improvement', 0)
                    episode_loss_rate = info.get('loss_rate', 100)
                    
                    if done:
                        break
                
                # è®°å½•å½“å‰episodeçš„ç»“æœ
                epoch_rewards.append(episode_reward)
                epoch_normalized_improvements.append(episode_normalized_improvement)
                epoch_loss_rates.append(episode_loss_rate)
                
                # æ‰“å°è¿›åº¦
                if (episode_in_sample + 1) % 5 == 0 or episode_in_sample == samples_per_epoch - 1:
                    baseline_loss = info.get('baseline_loss', 0)
                    relative_improvement = info.get('relative_improvement', 0)
                    print(f"Epoch {epoch+1}/{epochs}, Sample {sample_idx+1}/{len(sample_files)} ({sample_name}), "
                          f"Episode {episode_in_sample+1}/{samples_per_epoch}: "
                          f"Reward={episode_reward:.2f}, Loss={episode_loss_rate:.2f}%, "
                          f"åŸºå‡†={baseline_loss:.2f}%, ç›¸å¯¹æ”¹å–„={relative_improvement:.1f}%")
        
        # è®¡ç®—æœ¬è½®epochçš„å¹³å‡æ€§èƒ½
        avg_reward = np.mean(epoch_rewards) if epoch_rewards else 0
        avg_normalized_improvement = np.mean(epoch_normalized_improvements) if epoch_normalized_improvements else 0
        avg_loss_rate = np.mean(epoch_loss_rates) if epoch_loss_rates else 100
        
        rewards_history.append(avg_reward)
        normalized_improvements_history.append(avg_normalized_improvement)
        loss_rates_history.append(avg_loss_rate)
        
        # ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆåŸºäºå½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦ï¼‰
        best_normalized_improvement = save_best_normalized_model(
            agent=agent,
            filename="M1_normalized",
            best_normalized_improvement=best_normalized_improvement,
            current_normalized_improvement=avg_normalized_improvement,
            paths=paths
        )
        
        # æ‰“å°epochæ€»ç»“
        print(f"\n=== Epoch {epoch+1}/{epochs} å®Œæˆ ===")
        print(f"å¹³å‡å¥–åŠ±: {avg_reward:.2f}")
        print(f"å¹³å‡å½’ä¸€åŒ–æ”¹å–„: {avg_normalized_improvement:.2f}%")
        print(f"å¹³å‡ç½‘æŸç‡: {avg_loss_rate:.4f}%")
        print(f"å½“å‰æœ€ä¼˜å½’ä¸€åŒ–æ”¹å–„: {best_normalized_improvement:.2f}%")
        print("-" * 60)
    
    # -------------------------- 6. ç»˜åˆ¶è®­ç»ƒæ›²çº¿ --------------------------
    plt.rcParams["font.family"] = ["Heiti TC", "sans-serif"]
    plt.rcParams["axes.unicode_minus"] = False
    
    fig, axes = plt.subplots(1, 3, figsize=(15, 4))
    
    # å¥–åŠ±æ›²çº¿
    axes[0].plot(rewards_history, alpha=0.7, marker='o', markersize=3)
    axes[0].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)')
    axes[0].set_ylabel('å¹³å‡å¥–åŠ±')
    axes[0].set_title('å¤šæ ·æœ¬è®­ç»ƒå¥–åŠ±æ›²çº¿')
    axes[0].grid(True)
    
    # å½’ä¸€åŒ–æ”¹å–„æ›²çº¿
    axes[1].plot(normalized_improvements_history, alpha=0.7, marker='o', markersize=3, color='green')
    axes[1].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)')
    axes[1].set_ylabel('å¹³å‡å½’ä¸€åŒ–æ”¹å–„ (%)')
    axes[1].set_title('å½’ä¸€åŒ–æ”¹å–„ç¨‹åº¦å˜åŒ–')
    axes[1].grid(True)
    
    # ç½‘æŸç‡æ›²çº¿
    axes[2].plot(loss_rates_history, alpha=0.7, marker='o', markersize=3, color='red')
    axes[2].set_xlabel('è®­ç»ƒè½®æ¬¡ (Epoch)')
    axes[2].set_ylabel('å¹³å‡ç½‘æŸç‡ (%)')
    axes[2].set_title('ç½‘æŸç‡å˜åŒ–')
    axes[2].grid(True)
    
    plt.tight_layout()
    plt.savefig('training_curve_normalized_improvement.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # -------------------------- 7. åœ¨æ‰€æœ‰æ ·æœ¬ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹ --------------------------
    print("\n=== åœ¨æ‰€æœ‰æ ·æœ¬ä¸Šè¯„ä¼°æœ€ç»ˆæ¨¡å‹ ===")
    
    evaluation_results = []
    for sample_idx, sample_file in enumerate(sample_files):
        sample_name = os.path.basename(sample_file)
        
        env = sample_manager.create_env_for_sample(
            sample_file, branch_data, voltage_lower, voltage_upper, key_nodes
        )
        
        state = env.reset()
        episode_reward = 0
        episode_normalized_improvement = 0
        episode_loss_rate = 0
        baseline_loss = env.baseline_info['baseline_loss']
        
        for step in range(max_steps):
            # æµ‹è¯•æ—¶ä¸åŠ æ¢ç´¢å™ªå£°
            action = agent.select_action(state, exploration_noise=0)
            next_state, reward, done, info = env.step(action)
            
            state = next_state
            episode_reward += reward
            episode_normalized_improvement = info.get('normalized_improvement', 0)
            episode_loss_rate = info.get('loss_rate', 100)
            
            if done:
                break
        
        relative_improvement = ((baseline_loss - episode_loss_rate) / baseline_loss * 100) if baseline_loss > 0 else 0
        
        evaluation_results.append({
            'sample': sample_name,
            'reward': episode_reward,
            'loss_rate': episode_loss_rate,
            'baseline_loss': baseline_loss,
            'relative_improvement': relative_improvement,
            'normalized_improvement': episode_normalized_improvement
        })
        
        print(f"æ ·æœ¬ {sample_idx+1}/{len(sample_files)} ({sample_name}):")
        print(f"  åŸºå‡†ç½‘æŸç‡: {baseline_loss:.4f}%")
        print(f"  ä¼˜åŒ–åç½‘æŸç‡: {episode_loss_rate:.4f}%")
        print(f"  ç›¸å¯¹æ”¹å–„: {relative_improvement:.2f}%")
        print(f"  å½’ä¸€åŒ–æ”¹å–„: {episode_normalized_improvement:.2f}%")
        print(f"  å¥–åŠ±: {episode_reward:.2f}")
        print()
    
    # è®¡ç®—å¹³å‡æ€§èƒ½
    avg_test_reward = np.mean([r['reward'] for r in evaluation_results])
    avg_test_loss_rate = np.mean([r['loss_rate'] for r in evaluation_results])
    avg_relative_improvement = np.mean([r['relative_improvement'] for r in evaluation_results])
    avg_normalized_improvement = np.mean([r['normalized_improvement'] for r in evaluation_results])
    
    print(f"\n=== æœ€ç»ˆè¯„ä¼°ç»“æœ ===")
    print(f"å¹³å‡å¥–åŠ±: {avg_test_reward:.2f}")
    print(f"å¹³å‡ç½‘æŸç‡: {avg_test_loss_rate:.4f}%")
    print(f"å¹³å‡ç›¸å¯¹æ”¹å–„: {avg_relative_improvement:.2f}%")
    print(f"å¹³å‡å½’ä¸€åŒ–æ”¹å–„: {avg_normalized_improvement:.2f}%")
    print(f"æœ€ä¼˜å½’ä¸€åŒ–æ”¹å–„: {best_normalized_improvement:.2f}%")
    print(f"ä»…ä¿ç•™æœ€ä¼˜æ¨¡å‹æ–‡ä»¶ï¼Œéæœ€ä¼˜æ¨¡å‹å·²è‡ªåŠ¨åˆ é™¤")
    
    # ä¿å­˜è¯„ä¼°ç»“æœåˆ°CSVæ–‡ä»¶
    results_df = pd.DataFrame(evaluation_results)
    results_df.to_csv('evaluation_results_normalized.csv', index=False, encoding='utf-8-sig')
    print(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: evaluation_results_normalized.csv")

# -------------------------- ä¸»å‡½æ•° --------------------------
if __name__ == "__main__":
    try:
        # ä½¿ç”¨åŸºäºå½’ä¸€åŒ–æ”¹å–„çš„è®­ç»ƒå‡½æ•°
        train_rl_model_with_normalized_improvement()
    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        raise